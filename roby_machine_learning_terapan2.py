# -*- coding: utf-8 -*-
"""Roby_machine_learning_terapan2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15a8Nb5lGvxGEv4zyQyvovraRlMoLOoEB

#importlibrary
"""

from google.colab import drive
import zipfile
import pandas as pd
import pickle
from collections import defaultdict
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.dummy import DummyRegressor
import matplotlib.pyplot as plt

"""Mengimpor library yang diperlukan untuk:

Google Colab: Akses Google Drive
pandas/numpy: Manipulasi data
TensorFlow: Membangun neural network
sklearn: Preprocessing dan evaluasi model

#load dataset
"""

# Mount Google Drive
drive.mount('/content/drive')

# Extract dataset
zip_path = '/content/drive/MyDrive/Content based filtering/archive (4).zip'
extract_path = '/content/content_based_filtering'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)
print("Dataset extracted successfully!")

# Define file paths
csv_path = '/content/content_based_filtering/content_movie_list.csv'
pickle_path = '/content/content_based_filtering/content_user_to_genre.pickle'
header_path = '/content/content_based_filtering/content_user_train_header.txt'

# Load data files
movie_list = pd.read_csv(csv_path)

# Load user header
with open(header_path, 'r') as header:
    for line in header:
        user_header = line.strip().split(',')

# Load user-genre preferences
with open(pickle_path, 'rb') as f:
    user_to_genre = pickle.load(f)

print("Data loaded successfully!")
print(f"Number of movies: {len(movie_list)}")
print(f"Number of users: {len(user_to_genre)}")

"""Memuat dataset yang terdiri dari:

movie_list: Daftar film dengan ID, judul, dan genre
user_to_genre: Dictionary berisi preferensi user terhadap genre dan rating yang diberikan

#understanding data
"""

user_to_genre[2]

"""Menunjukkan struktur data user yang berisi:

glist: Rating rata-rata user untuk setiap genre
g_count: Jumlah film yang dinilai per genre
rating_count: Total jumlah rating
movies: Dictionary film yang dinilai dengan rating-nya
rating_ave: Rating rata-rata user


"""

movie_list

movie_list.isna().sum()

movie_list.duplicated().sum()

movie_list.head()

"""#Finding average ratings for all movies"""

#creating sum of total rating col and count of total rating col
movie_list['total_rating_sum']=0
movie_list['total_rating_count']=0

#Disable warnings
pd.options.mode.chained_assignment = None  # default='warn'

#Calculating total ratings for each movies by looping through all users who gave ratings
for user in user_to_genre:
    #print(user_to_genre[user])
    #print(user['movies'])
    user_movies=user_to_genre[user]['movies']
    for movie in user_movies:
        #print(movie,user_movies[movie])
        movie_list.loc[:,'total_rating_sum'][movie_list['movieId']==movie]+=user_movies[movie]
        movie_list.loc[:,'total_rating_count'][movie_list['movieId']==movie]+=1

"""Menghitung rating rata-rata untuk setiap film dari semua user yang memberikan rating."""

#Average rating =total_ratings/total_ratings_count
movie_list.loc[:,'avg rating']=movie_list.loc[:,'total_rating_sum'] / movie_list.loc[:,'total_rating_count']

#Deleting unwanted columns
movie_list.drop(['total_rating_sum','total_rating_count'],axis=1,inplace=True)

movie_list

my_item_vec=pd.DataFrame()
my_item_vec

"""#spliting genres"""

# Create an empty list to store the data for the new DataFrame
data_list = []

for row in movie_list.values:
    categories=row[2].split('|')
    #print(categories)
    for category in categories:
        # Append a dictionary representing the row to the list
        data_list.append({'movieId':row[0],'year':row[1][-5:-1],'ave rating':row[3],'title':row[1][:-7],'genres':category})

# Create the DataFrame from the list of dictionaries
my_item_vec = pd.DataFrame(data_list)

my_item_vec['movieId']=my_item_vec['movieId'].astype(int)

my_item_vec

movie_list_seperated_genres=my_item_vec.copy()

"""#One Hot Encoding generes column"""

#one hot encoding generes column
from sklearn.preprocessing import OneHotEncoder
# Remove the sparse=False argument if your scikit-learn version is older
ohe_categories=OneHotEncoder(handle_unknown='ignore')
# Convert the sparse matrix output to a dense array before creating the DataFrame
my_item_vec_categories=pd.DataFrame(ohe_categories.fit_transform(my_item_vec['genres'].to_numpy().reshape(-1,1)).toarray()
                         ,columns=[col[3:] for col in ohe_categories.get_feature_names_out()]).astype(int)

"""Mengubah format data film:

Memisahkan film dengan multiple genre menjadi baris terpisah
Mengubah genre menjadi format one-hot encoding (0/1)
"""

my_item_vec=pd.concat([my_item_vec,my_item_vec_categories],axis=1)
my_item_vec.head()

#Dropping unwanted columns
my_item_vec.drop(['title','genres'],axis=1,inplace=True)

my_item_vec.head()

"""#Making training set : users and items"""

users=pd.DataFrame([],columns=user_header)
items=pd.DataFrame([],columns=my_item_vec.columns)
y=pd.DataFrame([]) #actual ratings given by users

#We are creating training data so that there is 1 user that who has given rating to 1 item

for user in user_to_genre:


    #get all movie ids in list
    movie_dict=user_to_genre[user]['movies']

    #select data of all movies with above movie_ids in my_item_vec
    all_movies=my_item_vec[my_item_vec['movieId'].isin(movie_dict)].reset_index(drop=True)

    #select the ratings that user actually gave to the movies
    user_ratings=all_movies['movieId'].apply(lambda all_movies_movie_id:movie_dict[all_movies_movie_id])  #returns ratings given to movie by this user

    #Combining to original training data
    items=pd.concat([items,all_movies],axis=0,ignore_index=True)
    y=pd.concat([y,user_ratings],axis=0,ignore_index=True)
    #glist contains avg rating of user for different categories
    #combining user id, rating count and rating ave with glist
    combined_arr=np.c_[np.array([[user,user_to_genre[user]['rating_count'],user_to_genre[user]['rating_ave']]]),user_to_genre[user]['glist']]

    #No of movies rated
    num_repeat=all_movies.shape[0]

    #Repeat user data equal to no. of movie data the user has rated
    combined_arr_repeated=np.tile(combined_arr,(num_repeat,1))


    #Combining in original training data
    users=pd.concat([users,pd.DataFrame(combined_arr_repeated,columns=user_header)],axis=0,ignore_index=True)

"""Membuat dataset training dengan format:

users: Fitur user (preferensi genre, rating count, dll)
items: Fitur film (tahun, rating rata-rata, genre one-hot)
y: Target (rating aktual yang diberikan user)
"""

#Converting year from object to float
items['year']=items['year'].astype(int)

users.head()

items.head()

y.head()

"""#Processing training data"""

#In model we will not use first 3 columns from from users set i.e 'user id','rating count' and 'rating ave'
user_features=[col for col in users.columns if col not in ['user id','rating count','rating ave'] ]
num_user_features=len(user_features)
user_features_start=3   #Model takes values from 3rd column to last column

#In model we will not use first column from items set i.e col 'movieId'
item_features=[col for col in items.columns if col not in ['movieId'] ]
num_item_features=len(item_features)
item_features_start=1   #Model takes values from 1st column to last column

print('User features : ',num_user_features,' : ',user_features)
print('Item features : ',num_item_features,' : ',item_features)

"""##Scaling"""

#Scaling data
from sklearn.preprocessing import StandardScaler

scaledata=True
if scaledata:
    item_train_save = items.copy()
    user_train_save = users.copy()

    scaler_items=StandardScaler()
    items=pd.DataFrame(scaler_items.fit_transform(items),columns=items.columns)

    scaler_users=StandardScaler()
    users=pd.DataFrame(scaler_users.fit_transform(users),columns=users.columns)

"""StandardScaler: Menormalkan fitur user dan item
MinMaxScaler: Mengubah target rating ke range [-1,1]
"""

#Splitting training and test data
items_train,items_test=train_test_split(items,test_size=0.2,shuffle=True,random_state=1)
users_train,users_test=train_test_split(users,test_size=0.2,shuffle=True,random_state=1)
y_train,y_test=train_test_split(y,test_size=0.2,shuffle=True,random_state=1)

from sklearn.preprocessing import MinMaxScaler

y_scaler=MinMaxScaler((-1,1))
y_train_norm=y_scaler.fit_transform(y_train)
y_test_norm=y_scaler.transform(y_test)

"""#model"""

#Making Model
num_outputs=32
tf.random.set_seed(1)
users_NN=tf.keras.models.Sequential([
    tf.keras.layers.Dense(256,activation='relu'),
    tf.keras.layers.Dense(128,activation='relu'),
    tf.keras.layers.Dense(num_outputs,activation='linear'),
])

items_NN=tf.keras.models.Sequential([
    tf.keras.layers.Dense(256,activation='relu'),
    tf.keras.layers.Dense(128,activation='relu'),
    tf.keras.layers.Dense(num_outputs,activation='linear'),
])

class L2NormalizeLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        return tf.linalg.l2_normalize(inputs, axis=1)


users_input=tf.keras.layers.Input(shape=tuple([num_user_features]))
vu=users_NN(users_input)
# Apply L2 normalization using the custom layer
vu=L2NormalizeLayer()(vu)

items_input=tf.keras.layers.Input(shape=tuple([num_item_features]))
vm=items_NN(items_input)
# Apply L2 normalization using the custom layer
vm=L2NormalizeLayer()(vm)

# compute the dot product of the two vectors vu and vm
output=tf.keras.layers.Dot(axes=1)([vu,vm])

#Specify input and output of model
model=tf.keras.Model([users_input,items_input],output)

model.summary()

"""Sekarang, mari kita buat jaringan syaraf seperti yang dijelaskan pada gambar di atas. Ini akan memiliki dua jaringan yang digabungkan dengan dot product. Anda akan membangun dua jaringan. Dalam contoh ini, kedua jaringan tersebut akan identik. Perhatikan bahwa jaringan-jaringan ini tidak harus sama. Jika konten pengguna secara substansial lebih besar daripada konten film, Anda dapat memilih untuk meningkatkan kompleksitas jaringan pengguna relatif terhadap jaringan film. Dalam hal ini, kontennya serupa, sehingga jaringannya pun sama.

Gunakan model sekuensial Keras Lapisan pertama adalah lapisan padat dengan 256 unit dan aktivasi relu. Lapisan kedua adalah lapisan padat dengan 128 unit dan aktivasi relu. Lapisan ketiga adalah lapisan padat dengan unit num_outputs dan aktivasi linier atau tanpa aktivasi.

Translated with DeepL.com (free version)
"""

tf.random.set_seed(1)
cost_fun=tf.keras.losses.MeanSquaredError()
optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)
model.compile(loss=cost_fun,optimizer=optimizer)

model.fit([users_train[user_features],items_train[item_features]],y_train_norm,epochs=30)

model.evaluate([users_test[user_features],items_test[item_features]],y_test_norm)

"""#Predictions"""

def predict_user_ratings(model,user_vec,my_item_vec,user_features_start,item_features_start,scaler_items,scaler_users,y_scaler,scaledata):

    #All the movies are in my_item_vec
    #We will populate user_vec so that its no. of rows is equal to no. of rows in my_item_vec
    user_vecs=np.tile(user_vec,(my_item_vec.shape[0],1))

    #Scaling data if scaledata = True
    if scaledata:
        user_vecs=scaler_users.transform(user_vecs)
        item_vecs=scaler_items.transform(my_item_vec)

    #predict rating
    y_p=model.predict([user_vecs[:,user_features_start:],item_vecs[:,item_features_start:]])

    #inverse transform predicted rantings to get actual ratings
    y_p=y_scaler.inverse_transform(y_p)

    return y_p

"""#Predictions for a new use"""

new_user_id = 5000
new_rating_ave = 1.0
new_action = 1.0
new_adventure = 1
new_animation = 1
new_childrens = 1
new_comedy = 5
new_crime = 1
new_documentary = 1
new_drama = 1
new_fantasy = 1
new_horror = 1
new_mystery = 1
new_romance = 5
new_scifi = 5
new_thriller = 1
new_rating_count = 3

user_vec = np.array([[new_user_id, new_rating_count, new_rating_ave,
                      new_action, new_adventure, new_animation, new_childrens,
                      new_comedy, new_crime, new_documentary,
                      new_drama, new_fantasy, new_horror, new_mystery,
                      new_romance, new_scifi, new_thriller]])

#For new user we will predict rating for all the movies
y_p=predict_user_ratings(model,user_vec,my_item_vec,user_features_start,item_features_start,scaler_items,scaler_users,y_scaler,scaledata=scaledata)

movie_list_seperated_genres['y_p']=y_p
movie_list_seperated_genres.sort_values(by='y_p',ascending=False)

"""#Prediction for existing user"""

#get ratings for given user id
user_id=36

old_user_vec=scaler_users.inverse_transform(users_train)[scaler_users.inverse_transform(users_train)[:,0]==user_id][0]
print(old_user_vec)

# predict rating for all the movies
y_p=predict_user_ratings(model,old_user_vec,my_item_vec,user_features_start,item_features_start,scaler_items,scaler_users,y_scaler,scaledata=scaledata)

movie_list_seperated_genres['y_p']=y_p
movie_list_seperated_genres['y_actual']=0

for i in user_to_genre[user_id]['movies']:
    movie_list_seperated_genres['y_actual'][movie_list_seperated_genres['movieId']==i]=user_to_genre[user_id]['movies'][i]

movie_list_seperated_genres[movie_list_seperated_genres['y_actual']!=0].sort_values(by='y_p',ascending=False)

#y_actual is the actual rating of user and y_p is predicted rating

movie_list_seperated_genres.drop(['y_p','y_actual'],axis=1,inplace=True)

"""Jaringan syaraf di atas menghasilkan dua vektor fitur, vektor fitur pengguna Vu, dan vektor fitur film Vm. Keduanya merupakan 32 vektor entri yang nilainya sulit ditafsirkan. Namun, item yang serupa akan memiliki vektor yang serupa. Informasi ini dapat digunakan untuk membuat rekomendasi. Sebagai contoh, jika seorang pengguna memberi nilai tinggi pada “Toy Story 3”, seseorang dapat merekomendasikan film yang serupa dengan memilih film yang memiliki vektor fitur film yang serupa.
Ukuran kemiripan adalah jarak kuadrat antara dua vektor

Translated with DeepL.com (free version)

#Getting feature vector for movies : Vm

Matriks jarak antar film dapat dihitung sekali saat model dilatih dan kemudian digunakan kembali untuk rekomendasi baru tanpa pelatihan ulang. Langkah pertama, setelah model dilatih, adalah mendapatkan vektor fitur film Vm, untuk masing-masing film. Untuk melakukan ini, kita akan menggunakan item_NN yang telah dilatih dan membangun model kecil untuk memungkinkan kita menjalankan vektor film melaluinya untuk menghasilkan Vm.


Translated with DeepL.com (free version)
"""

input_item_m = tf.keras.layers.Input(shape=(num_item_features,))
vm_m = items_NN(input_item_m)
# Apply L2 normalization using the custom layer
vm_m = L2NormalizeLayer()(vm_m)

model_m = tf.keras.Model(input_item_m, vm_m)
model_m.summary()

scaled_my_item_vec=scaler_items.transform(my_item_vec)
vms=model_m.predict(scaled_my_item_vec[:,item_features_start:])
vms.shape

"""#Recommending movies with minimum squared distance"""

dist=np.zeros((vms.shape[0],vms.shape[0]))

#Calculating squared distance from each movie to every other movie
for i in range(len(vms)):
    dist[i]=np.sum(np.square(vms[i]-vms),axis=1).reshape((1,-1))

dist

#masking so that we dont consider distance from a movie to itself
masked_dist=np.ma.masked_array(dist,mask=np.identity(dist.shape[0]))
masked_dist

#For each row, getting index of movie with min distance
min_dist_movie_index=np.argmin(masked_dist,axis=1)
min_dist_movie_index

recommend_df=pd.DataFrame()
recommend_df['movie 1']=movie_list_seperated_genres['title'] +' ( '+movie_list_seperated_genres['year']+' ) '
recommend_df['movie 1 genres']=movie_list_seperated_genres['genres']

recommend_df['movie 2']=( movie_list_seperated_genres.loc[min_dist_movie_index,'title'] +' ( '+movie_list_seperated_genres.loc[min_dist_movie_index,'year']+' ) ' ).values
recommend_df['movie 2 genres']=movie_list_seperated_genres.loc[min_dist_movie_index,'genres'].values

recommend_df.head(10)